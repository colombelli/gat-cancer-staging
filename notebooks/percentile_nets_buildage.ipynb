{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e58805-5bc0-4d7d-ba0c-caf59130ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import snf\n",
    "from sklearn.cluster import spectral_clustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad088534-2d70-4eef-81b6-a81d728a49b2",
   "metadata": {},
   "source": [
    "### Different networks data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9a796e-936f-461a-8b3c-9075c489a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_dataframe(df):\n",
    "    col_1 = []\n",
    "    col_2 = []\n",
    "    col_r = []\n",
    "    col_p = []\n",
    "    \n",
    "    for idx1, row1 in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        for idx2, row2 in df.loc[idx1:, :].iterrows():\n",
    "            r, p = stats.pearsonr(row1.values, row2.values)\n",
    "            col_1.append(idx1)\n",
    "            col_2.append(idx2)\n",
    "            col_r.append(r)\n",
    "            col_p.append(p)\n",
    "            \n",
    "    corr_df = pd.DataFrame.from_dict({\n",
    "        \"sample1\": col_1,\n",
    "        \"sample2\": col_2,\n",
    "        \"r\": col_r,\n",
    "        \"p\": col_p\n",
    "    })\n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def merge_correlation_dataframes(dfs):\n",
    "    \n",
    "    greatest_r = np.argmax(np.array([df.r for df in dfs]), axis=0)\n",
    "    to_concat = [df.loc[greatest_r == i] for i, df in enumerate(dfs)]\n",
    "    return pd.concat(to_concat).sort_index()\n",
    "\n",
    "\n",
    "def build_edge_list(df, r_filter, p_filter):\n",
    "    edges_df = df.loc[(df.r >= r_filter) & (df['p'] <= p_filter)]\n",
    "    return edges_df.rename(columns={'sample1':'source', 'sample2':'target'})\n",
    "\n",
    "def filter_relevant_connections(df, threshold):\n",
    "    return df.loc[(df.weight >= threshold)]\n",
    "\n",
    "\n",
    "def get_stage_class_from_patient(patient_idx, clin_df, agglutinate_stages=False):\n",
    "    stage_str = clin_df.loc[patient_idx, \"pathologic_stage\"]\n",
    "    \n",
    "    if stage_str in [\"stage i\"+suffix for suffix in ['', 'a','b','c']]:\n",
    "        return \"stage1\"\n",
    "    elif stage_str in [\"stage ii\"+suffix for suffix in ['', 'a','b','c']]:\n",
    "        if agglutinate_stages:\n",
    "            return \"stage23\"\n",
    "        else:\n",
    "            return \"stage2\"\n",
    "    elif stage_str in [\"stage iii\"+suffix for suffix in ['', 'a','b','c']]:\n",
    "        if agglutinate_stages:\n",
    "            return \"stage23\"\n",
    "        else:\n",
    "            return \"stage3\"\n",
    "    elif stage_str in [\"stage iv\"+suffix for suffix in ['', 'a','b','c']]:\n",
    "        return \"stage4\"\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def build_class_df(sample_idxs, agglutinate_stages=False):\n",
    "    \n",
    "    clin_df = pd.read_csv(f\"{base}{cancer}_clin.txt\", sep=\"\\t\", index_col=0).T.iloc[:, [6]]\n",
    "    \n",
    "    class_col = []\n",
    "    for idx in sample_idxs:\n",
    "        patient_idx = '-'.join(idx.split('.')[:-1]).lower()\n",
    "        sample_type = int(idx.split('.')[-1])\n",
    "        \n",
    "        if sample_type <= 9:   # Tumor sample\n",
    "            class_col.append(get_stage_class_from_patient(patient_idx, clin_df, agglutinate_stages))\n",
    "        elif sample_type <= 19:   # Normal sample\n",
    "            class_col.append('normal')\n",
    "        elif sample_type <= 29:   # Control sample\n",
    "            print(f\"Warning! Found control sample {idx}, Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Warning! Found unexpected sample type: {idx}. Skipping...\")\n",
    "                  \n",
    "    return pd.DataFrame.from_dict({\n",
    "            \"id\": sample_idxs,\n",
    "            \"class\": class_col\n",
    "        }).set_index(\"id\")\n",
    "\n",
    "\n",
    "def get_consistency_index(corr_df, class_df):\n",
    "    correct_connections = 0\n",
    "    for index, row in corr_df.iterrows():\n",
    "        src_class = class_df.loc[row[0], \"class\"]\n",
    "        trg_class = class_df.loc[row[1], \"class\"]\n",
    "        \n",
    "        if src_class == trg_class:\n",
    "            correct_connections += 1\n",
    "    return correct_connections/len(corr_df)\n",
    "\n",
    "\n",
    "def generate_csvs(edges_df, class_df, max_each_feature=100, multi_omics=True):\n",
    "    \n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    edges_df.to_csv(save_dir+\"edges.csv\", index=False)\n",
    "    class_df.to_csv(save_dir+\"classes.csv\", index=True)\n",
    "    \n",
    "    gene = pd.read_csv(f\"{base}{cancer}_mRNA.csv\", index_col=0).iloc[:max_each_feature, :]\n",
    "    if multi_omics:\n",
    "        mirna = pd.read_csv(f\"{base}{cancer}_miRNA.csv\", index_col=0).iloc[:max_each_feature, :]\n",
    "        meth = pd.read_csv(f\"{base}{cancer}_Methy.csv\", index_col=0).iloc[:max_each_feature, :] \n",
    "        #cnv = pd.read_csv(f\"{base}{cancer}_CNV.csv\", index_col=0).iloc[:max_each_feature, :]\n",
    "        #features_df = pd.concat([gene,mirna,meth,cnv]).T\n",
    "        features_df = pd.concat([gene,mirna,meth]).T\n",
    "    else:\n",
    "        features_df = gene.T\n",
    "    \n",
    "    features_df.loc[class_df.index, :].to_csv(save_dir+\"features.csv\", index=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_snf_network(dfs_values_matrices, class_df):\n",
    "    affinity_networks = snf.make_affinity(dfs_values_matrices)\n",
    "    fused_network = snf.snf(affinity_networks)\n",
    "    np.fill_diagonal(fused_network, 1)\n",
    "    G = nx.from_pandas_adjacency(pd.DataFrame(data=fused_network, index=class_df.index.values, columns=class_df.index.values), create_using=nx.Graph())\n",
    "    return nx.to_pandas_edgelist(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8359e27d-bb8c-4776-b633-6ceea554d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD\n",
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "KIRC\n",
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "LUAD\n",
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancers = [\"COAD\", \"KIRC\", \"LUAD\"]\n",
    "percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "for cancer in cancers:\n",
    "    print(cancer)\n",
    "    base = f\"C:/Users/colombelli/Desktop/TCC/experiments/{cancer}/\"\n",
    "\n",
    "    dfs_values = []\n",
    "\n",
    "    df1 = pd.read_csv(f\"{base}{cancer}_mRNA.csv\", index_col=0).T\n",
    "    class_df = build_class_df(list(df1.index), agglutinate_stages=False).dropna()\n",
    "    dfs_values.append(df1.loc[class_df.index, :].values)\n",
    "\n",
    "    df2 = pd.read_csv(f\"{base}{cancer}_miRNA.csv\", index_col=0).T\n",
    "    dfs_values.append(df2.loc[class_df.index, :].values)\n",
    "\n",
    "    df3 = pd.read_csv(f\"{base}{cancer}_Methy.csv\", index_col=0).T\n",
    "    dfs_values.append(df3.loc[class_df.index, :].values)\n",
    "    \n",
    "    graph_df = get_snf_network(dfs_values, class_df)\n",
    "    \n",
    "    # Write class counts and their weights for the categorical cross entropy\n",
    "    n_samples = len(class_df)\n",
    "    n_classes = 4\n",
    "    with open(f\"{base}class_info.txt\", 'w') as f:\n",
    "        print(\"class\\t | #\\t| weight\", file=f)\n",
    "        for i, count in class_df.value_counts().items():\n",
    "            weight = n_samples / (n_classes * count)\n",
    "            print(i[0], \"\\t |\", count, \"\\t| \", weight, file=f)\n",
    "    \n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(\"Percentile: \", percentile)\n",
    "        th = graph_df['weight'].quantile(percentile)\n",
    "        save_dir = f\"{base}snf/{str(percentile).replace('.', '')}/\"\n",
    "        generate_csvs(filter_relevant_connections(graph_df, th), class_df, max_each_feature=999999)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7dc875-c80b-474f-9548-477c3292cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 282/282 [00:03<00:00, 88.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 282/282 [00:02<00:00, 99.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 282/282 [00:03<00:00, 86.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "KIRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 76.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 84.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 76.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 445/445 [00:08<00:00, 52.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 445/445 [00:07<00:00, 59.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 445/445 [00:08<00:00, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancers = [\"COAD\", \"KIRC\", \"LUAD\"]\n",
    "percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "for cancer in cancers:\n",
    "    print(cancer)\n",
    "    base = f\"C:/Users/colombelli/Desktop/TCC/experiments/{cancer}/\"\n",
    "\n",
    "    df = pd.read_csv(f\"{base}{cancer}_mRNA.csv\", index_col=0).T\n",
    "    class_df = build_class_df(list(df.index), agglutinate_stages=False).dropna()\n",
    "    df = df.loc[class_df.index, :]\n",
    "    corr_df_mrna = get_correlation_dataframe(df) \n",
    "\n",
    "    df = pd.read_csv(f\"{base}{cancer}_miRNA.csv\", index_col=0).T\n",
    "    df = df.loc[class_df.index, :]\n",
    "    corr_df_mirna = get_correlation_dataframe(df) \n",
    "\n",
    "    df = pd.read_csv(f\"{base}{cancer}_Methy.csv\", index_col=0).T\n",
    "    df = df.loc[class_df.index, :]\n",
    "    corr_df_methy = get_correlation_dataframe(df) \n",
    "\n",
    "    corr_df = merge_correlation_dataframes([corr_df_mrna, corr_df_mirna, corr_df_methy])\n",
    "    \n",
    "    # Write class counts and their weights for the categorical cross entropy\n",
    "    n_samples = len(class_df)\n",
    "    n_classes = 4\n",
    "    with open(f\"{base}class_info.txt\", 'w') as f:\n",
    "        print(\"class\\t | #\\t| weight\", file=f)\n",
    "        for i, count in class_df.value_counts().items():\n",
    "            weight = n_samples / (n_classes * count)\n",
    "            print(i[0], \"\\t |\", count, \"\\t| \", weight, file=f)\n",
    "    \n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(\"Percentile: \", percentile)\n",
    "        th = corr_df['r'].quantile(percentile)\n",
    "        save_dir = f\"{base}correlation_multi_omics/{str(percentile).replace('.', '')}/\"\n",
    "        generate_csvs(build_edge_list(corr_df, th, 0.05), class_df, max_each_feature=999999)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff656c4-4ca1-47f8-90e2-66d61e7742b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 282/282 [00:03<00:00, 82.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "KIRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:04<00:00, 74.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n",
      "LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 445/445 [00:08<00:00, 52.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile:  0.01\n",
      "Percentile:  0.05\n",
      "Percentile:  0.1\n",
      "Percentile:  0.25\n",
      "Percentile:  0.5\n",
      "Percentile:  0.75\n",
      "Percentile:  0.9\n",
      "Percentile:  0.95\n",
      "Percentile:  0.99\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancers = [\"COAD\", \"KIRC\", \"LUAD\"]\n",
    "percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "for cancer in cancers:\n",
    "    print(cancer)\n",
    "    base = f\"C:/Users/colombelli/Desktop/TCC/experiments/{cancer}/\"\n",
    "\n",
    "    df = pd.read_csv(f\"{base}{cancer}_mRNA.csv\", index_col=0).T\n",
    "    class_df = build_class_df(list(df.index), agglutinate_stages=False).dropna()\n",
    "    df = df.loc[class_df.index, :]\n",
    "    corr_df = get_correlation_dataframe(df)\n",
    "    \n",
    "    # Write class counts and their weights for the categorical cross entropy\n",
    "    n_samples = len(class_df)\n",
    "    n_classes = 4\n",
    "    with open(f\"{base}class_info.txt\", 'w') as f:\n",
    "        print(\"class\\t | #\\t| weight\", file=f)\n",
    "        for i, count in class_df.value_counts().items():\n",
    "            weight = n_samples / (n_classes * count)\n",
    "            print(i[0], \"\\t |\", count, \"\\t| \", weight, file=f)\n",
    "    \n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(\"Percentile: \", percentile)\n",
    "        th = corr_df['r'].quantile(percentile)\n",
    "        save_dir = f\"{base}correlation/{str(percentile).replace('.', '')}/\"\n",
    "        generate_csvs(build_edge_list(corr_df, th, 0.05), class_df, max_each_feature=999999)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63dd72-5d30-41e3-9382-a4f0c9b2ba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
